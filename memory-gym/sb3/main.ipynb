{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load autoreloader\n",
    "# %reload_ext autoreload\n",
    "# %load_ext tensorboard\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/sj1030/miniconda3/envs/memory-gym/lib/python3.11/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/common/home/sj1030/miniconda3/envs/memory-gym/lib/python3.11/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/common/home/sj1030/miniconda3/envs/memory-gym/lib/python3.11/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "2023-11-20 19:12:00.182136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-20 19:12:01.828025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/common/home/sj1030/miniconda3/envs/memory-gym/lib/python3.11/site-packages/tensorflow/python/debug/cli/debugger_cli_common.py:19: DeprecationWarning: module 'sre_constants' is deprecated\n",
      "  import sre_constants\n"
     ]
    }
   ],
   "source": [
    "# import memory_gym\n",
    "import gymnasium as gym, os, json\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm.notebook import tqdm\n",
    "from stable_baselines3.common.logger import Logger, HumanOutputFormat\n",
    "from stable_baselines3.common.vec_env.base_vec_env import (\n",
    "    VecEnv,\n",
    "    VecEnvStepReturn,\n",
    "    VecEnvWrapper,\n",
    ")\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat, configure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "os.sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from sb3 import model as model_lib\n",
    "\n",
    "from sb3 import helm as helm_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'MortarMayhem-v0',\n",
       " 'entry_point': 'memory_gym.mortar_mayhem:MortarMayhemEnv',\n",
       " 'reward_threshold': None,\n",
       " 'nondeterministic': False,\n",
       " 'max_episode_steps': None,\n",
       " 'order_enforce': True,\n",
       " 'autoreset': False,\n",
       " 'disable_env_checker': False,\n",
       " 'apply_api_compatibility': False,\n",
       " 'kwargs': {},\n",
       " 'additional_wrappers': [],\n",
       " 'vector_entry_point': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"memory_gym:MortarMayhem-v0\",\n",
    ")\n",
    "json.loads(env.spec.to_json())\n",
    "\n",
    "# # env = gym.make('MortarMayhem-v0', )\n",
    "# env_func = lambda: gym.make('MortarMayhem-v0', )\n",
    "# # envs = [gym.make('MortarMayhem-v0') for _ in range(4)]\n",
    "# env = VecNormalize(DummyVecEnv([env_func]*4))\n",
    "\n",
    "# # json.loads(env.spec.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(0.0, 1.0, (84, 84, 3), float32), MultiDiscrete([3 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicRewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, plot_freq=10, log_dir=\"./tensorboard_logs/\"):\n",
    "        super(EpisodicRewardCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.plot_freq = plot_freq\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_rollout_start(self) -> None:\n",
    "        # print(\"starting rollout\")\n",
    "        self.ep_rewards = 0.0\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.ep_rewards += self.locals[\"rewards\"][-1]\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # print(\"ending rollout\")\n",
    "        episode_reward = self.ep_rewards\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        # print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    # def _on_training_end(self) -> None:\n",
    "    #     self._plot_rewards()\n",
    "\n",
    "    # def _plot_rewards(self):\n",
    "    #     x = np.arange(1, len(self.episode_rewards) + 1)\n",
    "    #     plt.figure(figsize=(10, 5))\n",
    "    #     plt.plot(x, self.episode_rewards)\n",
    "    #     plt.xlabel('Episode')\n",
    "    #     plt.ylabel('Total Reward')\n",
    "    #     plt.title('Episodic Rewards')\n",
    "    #     plt.grid(True)\n",
    "    #     plt.savefig(os.path.join(self.log_dir, 'episodic_rewards.png'))\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_lr, final_lr, total_timesteps):\n",
    "    def schedule(progress):\n",
    "        fraction = min(progress / total_timesteps, 1.0)\n",
    "        return initial_lr + fraction * (final_lr - initial_lr)\n",
    "\n",
    "    return schedule\n",
    "\n",
    "\n",
    "def custom_entropy_schedule(initial_entropy_coef, final_entropy_coef, total_timesteps):\n",
    "    def entropy_coef_schedule(progress):\n",
    "        fraction = min(progress / total_timesteps, 1.0)\n",
    "        return torch.tensor(\n",
    "            initial_entropy_coef\n",
    "            + fraction * (final_entropy_coef - initial_entropy_coef),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "    return entropy_coef_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/sj1030/Projects/memory-gym/sb3/helm.py:162: UserWarning: You have specified a mini-batch size of 16384, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048\n",
      "We recommend using a `batch_size` that is a multiple of `n_steps * n_envs`.\n",
      "Info: (n_steps=2048 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "model = helm_lib.HELMPPO(\n",
    "    MultiInputActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=2,\n",
    "    # n_steps=total_timesteps,\n",
    "    n_epochs=3,\n",
    "    vf_coef=0.5,\n",
    "    clip_range=0.2,\n",
    "    batch_size=16384,\n",
    "    max_grad_norm=0.5,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.99,\n",
    "    # ent_coef=custom_entropy_schedule(1e-4, 1e-5, total_timesteps),\n",
    "    seed=1234,\n",
    "    tensorboard_log=\"/common/home/sj1030/Projects/memory-gym/sb3/tensorboard_logs/\",\n",
    "    clip_decay=\"none\",\n",
    "    _init_setup_model=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /common/home/sj1030/Projects/memory-gym/sb3/tensorboard_logs/HELM_14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c64eff7fba4af4b6db081fe09aa33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# callback = EpisodicRewardCallback(plot_freq=10, log_dir=\"tensorboard_logs/\")\n",
    "\n",
    "model.learn(total_timesteps=1000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"rewards.npy\")\n",
    "df = pd.DataFrame(arr, columns=[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSBOARD_SMOOTHING = [0.5, 0.85, 0.99]\n",
    "ts_factor = 0.99\n",
    "# for ts_factor in TSBOARD_SMOOTHING:\n",
    "smooth = df[2].ewm(alpha=(1 - ts_factor)).mean()\n",
    "plt.figure(figsize=(13, 5))\n",
    "# for ptx in range(3):\n",
    "# plt.subplot(1,3,ptx+1)\n",
    "plt.plot(df[2], alpha=0.1)\n",
    "plt.plot(smooth)\n",
    "plt.title(\"Mean Episodic Length\", fontdict={\"fontsize\": 20})\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(58.5, 64)\n",
    "\n",
    "label_interval = 200\n",
    "x_tick_positions = np.arange(0, len(df), label_interval)\n",
    "x_tick_labels = (x_tick_positions * 2e4).astype(int)\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(ticks=x_tick_positions, labels=x_tick_labels)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
